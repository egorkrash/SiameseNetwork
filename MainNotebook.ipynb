{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from gensim.models.wrappers.fasttext import FastTextKeyedVectors\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "data = pd.read_excel('task_10+kw_only_ru.xlsx', header=1)\n",
    "#data = pd.read_csv('new_apps_compatible.csv', sep=';').drop('Пустая колонка для совместимости', axis=1)\n",
    "fasttext_model_path = '/Users/egor/Downloads/187/model.model'\n",
    "\n",
    "#Параллельно выполняющийся map функции func по массиву massive\n",
    "def parallelization(func,massive, tq=True):\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count() # Число наших ядер\n",
    "    if tq:\n",
    "        results = np.array(Parallel(n_jobs=num_cores)(delayed(func)(i) for i in tqdm(massive)))\n",
    "        return results\n",
    "    else:\n",
    "        results = Parallel(n_jobs=num_cores)(delayed(func)(i) for i in massive)\n",
    "        return results\n",
    "\n",
    "def _word2canonical4w2v(word):\n",
    "    elems = morph.parse(word)\n",
    "    my_tag = ''\n",
    "    res = []\n",
    "    for elem in elems:\n",
    "        if 'VERB' in elem.tag or 'GRND' in elem.tag or 'INFN' in elem.tag:\n",
    "            my_tag = 'V'\n",
    "        if 'NOUN' in elem.tag:\n",
    "            my_tag = 'S'\n",
    "        normalised = elem.normalized.word\n",
    "        res.append((normalised, my_tag))\n",
    "    tmp = list(filter(lambda x: x[1] != '', res))\n",
    "    if len(tmp) > 0:\n",
    "        return tmp[0]\n",
    "    else:\n",
    "        return res[0]\n",
    "\n",
    "    \n",
    "def word2canonical(word):\n",
    "    return _word2canonical4w2v(word)[0]\n",
    "\n",
    "\n",
    "def getWords(text, filter_short_words=False):\n",
    "    if filter_short_words:\n",
    "        return filter(lambda x: len(x) > 3, re.findall(r'(?u)\\w+', text))\n",
    "    else:\n",
    "        return re.findall(r'(?u)\\w+', text)\n",
    "\n",
    "\n",
    "def text2canonicals(text, add_word=False, filter_short_words=True):\n",
    "    words = []\n",
    "    for word in getWords(text, filter_short_words=filter_short_words):\n",
    "        words.append(word2canonical(word.lower()))\n",
    "        if add_word:\n",
    "            words.append(word.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weight_matrix(word2vec, target_vocab, emb_dim=300):\n",
    "    matrix_len = len(target_vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            weights_matrix[i] = word2vec[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    \n",
    "    print(words_found, 'words found of', matrix_len, 'in target vocab')\n",
    "    return weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8225/8225 [02:40<00:00, 51.19it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = parallelization(text2canonicals, data.Core.values, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl.dump(texts, open('texts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastTextKeyedVectors.load(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = Counter([word for text in texts for word in text]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53158 words found of 53158 in target vocab\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = build_weight_matrix(fasttext, target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53158, 300)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = nn.Embedding(*weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs, num_embeddings, embedding_dim = create_emb_layer(weight_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2697, -0.5741, -1.5527, -2.2854, -1.2620, -0.5197,  3.0912,  0.7927,\n",
       "         -0.6073,  2.3412,  0.3576,  2.0995, -0.9263, -0.5159, -2.0464, -0.4677,\n",
       "         -2.9064, -0.0943,  0.6719,  2.4594,  1.6578, -2.9194, -0.0200,  1.2851,\n",
       "         -2.8199, -0.4918,  0.2292, -0.9744, -3.2194, -3.5566, -1.6003,  0.2985,\n",
       "          0.5833, -0.0737,  1.0696, -2.0138, -0.1739,  0.6468,  0.0755,  0.0971,\n",
       "         -0.1788,  2.0031,  0.4137, -2.9208,  1.8085,  1.6108,  1.3114, -1.9675,\n",
       "         -1.4978, -2.3921, -0.3813,  0.7139,  1.5572,  0.0862, -0.5874,  1.2259,\n",
       "         -2.1570,  0.3506,  0.5972,  0.7138,  2.2535, -0.2766,  0.2211, -0.9814,\n",
       "          0.5154, -0.7607,  0.3884,  3.5891,  1.7439,  0.7744,  1.3007,  1.2588,\n",
       "          1.2458, -1.9587,  0.3296,  1.2414, -1.3811, -2.7530, -0.3852,  1.8004,\n",
       "         -1.3537,  3.0254,  3.0503, -1.7619, -1.0446,  0.6383, -0.6296,  2.4570,\n",
       "          0.9960,  1.8724,  2.5050, -0.1670, -0.1157, -1.9065, -0.1082, -1.3729,\n",
       "          0.9684, -0.8983, -2.1295,  2.3596,  0.5286, -0.8339,  0.8847, -0.8947,\n",
       "         -0.2970, -2.9383, -2.0401, -0.1072,  0.1767, -0.9777,  0.1852,  0.0653,\n",
       "         -0.1692,  1.2026, -1.1932, -0.9227,  0.8569, -0.2739,  0.7042, -0.4209,\n",
       "         -1.8591,  2.7543, -1.4468, -1.2337,  0.9265, -0.1541,  2.6621,  3.0925,\n",
       "          3.4813, -0.3823, -4.6656, -0.3262,  0.6844,  3.2651,  3.7587,  2.6792,\n",
       "          1.8201,  0.1066,  1.9819, -3.4336,  0.0933,  1.7368,  1.6731, -1.6545,\n",
       "         -0.8069, -4.4324, -1.5860, -1.4034, -0.0092,  3.2547, -1.7804, -2.0134,\n",
       "         -0.3098,  0.5083,  1.2866,  0.7065,  2.4816,  0.6033, -1.5546,  1.6743,\n",
       "          0.7450, -0.8206,  2.0991,  0.8286, -0.8718,  1.5238, -2.6274, -3.3366,\n",
       "         -1.0323,  0.3302,  2.0419, -5.4879,  1.0910, -2.3857, -1.1168, -1.1759,\n",
       "         -3.0540, -2.6575,  1.8771, -0.8182, -3.2663,  0.6820, -0.3890,  3.4434,\n",
       "         -0.1065, -1.1168, -2.4792, -1.8577,  2.2806, -0.0117, -0.7818,  3.0726,\n",
       "          0.3839,  0.4122, -0.0732, -0.5188, -1.0816,  1.7733,  0.8768,  3.4274,\n",
       "         -2.4430, -1.7856,  1.2754,  0.0493, -1.0097, -2.0855,  0.2011, -2.6895,\n",
       "         -0.0430, -0.7537, -2.0447, -0.3346, -0.0890,  0.7783, -1.5845,  2.5534,\n",
       "          2.8786, -1.5956,  0.4594, -0.0194,  1.1815, -0.7911, -1.7289, -0.6680,\n",
       "          0.3703, -1.3628, -4.3842,  0.8236,  2.8749,  1.1459, -2.2303,  0.4468,\n",
       "         -3.1692,  1.0073, -0.9684, -1.6451,  1.2220, -0.8635, -0.6051, -3.5323,\n",
       "          1.2412, -0.9808,  0.3650, -0.6896,  0.0543, -0.1504,  0.3493, -0.2908,\n",
       "         -1.4240, -2.6101, -1.8228, -0.7322, -0.1610, -1.4315, -1.0791, -1.1471,\n",
       "         -0.0969,  0.8320,  3.4340,  0.7755, -3.3689,  1.2420, -0.0323, -2.1087,\n",
       "         -0.7937,  1.4400,  1.0089,  0.8324, -0.1046, -0.7837, -0.3487, -0.0556,\n",
       "         -0.8099, -2.2838, -0.6967,  0.9695, -0.5420,  1.8329, -1.4958, -1.6715,\n",
       "          0.1392,  2.0382,  1.0220,  0.7149, -0.3715,  1.7661, -1.5745, -0.3675,\n",
       "          0.7683,  0.5987,  0.5927,  1.5554, -1.1504,  1.0613, -1.7157,  0.5797,\n",
       "         -1.4331,  1.6347, -0.9814,  1.1417]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embeddings_path=None):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.embedding, num_embeddings, embedding_dim = self.create_emb_layer(torch.tensor(weights_matrix))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': torch.tensor(weights_matrix)})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "\n",
    "        return emb_layer, num_embeddings, embedding_dim\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        pass\n",
    "        \n",
    "    def get_embeddings(self):\n",
    "        assert self.embeddings_path is not None, 'you must specify embeddings path'\n",
    "        fasttext = FastTextKeyedVectors.load(self.embeddings_path)\n",
    "        The \n",
    "    \n",
    "    def get_loss(self):\n",
    "        # Implement pairwise loss\n",
    "        pass\n",
    "    \n",
    "    def get_prob(self, s1, s2):\n",
    "        return 1 / (1 + torch.exp(s1 - s2))\n",
    "    \n",
    "    def build(self):\n",
    "        # build arch of siamese network\n",
    "        self.input_embedding = nn.Embedding()\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
