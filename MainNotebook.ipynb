{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from gensim.models.wrappers.fasttext import FastTextKeyedVectors\n",
    "import torch\n",
    "from torch import nn\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "data = pd.read_excel('data/task_10+kw_only_ru.xlsx', header=1)\n",
    "#data = pd.read_csv('new_apps_compatible.csv', sep=';').drop('Пустая колонка для совместимости', axis=1)\n",
    "fasttext_model_path = '/Users/egor/Downloads/187/model.model'\n",
    "\n",
    "#Параллельно выполняющийся map функции func по массиву massive\n",
    "def parallelization(func, massive, jobs=3, tq=True):\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count() # Число наших ядер\n",
    "    if tq:\n",
    "        results = np.array(Parallel(n_jobs=num_cores)(delayed(func)(i) for i in tqdm(massive)))\n",
    "        return results\n",
    "    else:\n",
    "        results = Parallel(n_jobs=num_cores)(delayed(func)(i) for i in massive)\n",
    "        return results\n",
    "\n",
    "    \n",
    "def _word2canonical4w2v(word):\n",
    "    elems = morph.parse(word)\n",
    "    my_tag = ''\n",
    "    res = []\n",
    "    for elem in elems:\n",
    "        if 'VERB' in elem.tag or 'GRND' in elem.tag or 'INFN' in elem.tag:\n",
    "            my_tag = 'V'\n",
    "        if 'NOUN' in elem.tag:\n",
    "            my_tag = 'S'\n",
    "        normalised = elem.normalized.word\n",
    "        res.append((normalised, my_tag))\n",
    "    tmp = list(filter(lambda x: x[1] != '', res))\n",
    "    if len(tmp) > 0:\n",
    "        return tmp[0]\n",
    "    else:\n",
    "        return res[0]\n",
    "\n",
    "    \n",
    "def word2canonical(word):\n",
    "    return _word2canonical4w2v(word)[0]\n",
    "\n",
    "\n",
    "def getWords(text, filter_short_words=False):\n",
    "    if filter_short_words:\n",
    "        return filter(lambda x: len(x) > 3, re.findall(r'(?u)\\w+', text))\n",
    "    else:\n",
    "        return re.findall(r'(?u)\\w+', text)\n",
    "\n",
    "\n",
    "def text2canonicals(text, add_word=False, filter_short_words=True):\n",
    "    words = []\n",
    "    for word in getWords(text, filter_short_words=filter_short_words):\n",
    "        words.append(word2canonical(word.lower()))\n",
    "        if add_word:\n",
    "            words.append(word.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weight_matrix(word2vec, target_vocab, emb_dim=300):\n",
    "    matrix_len = len(target_vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            weights_matrix[i] = word2vec[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    \n",
    "    return weights_matrix\n",
    "\n",
    "\n",
    "def get_vocab(texts):\n",
    "    return Counter([word for text in texts for word in text]).keys()\n",
    "\n",
    "\n",
    "def get_queries_vocab(queries):\n",
    "    return Counter([word for qs in queries for qury in qs for word in qury]).keys()\n",
    "\n",
    "\n",
    "def text_to_idx(text, word_idx):\n",
    "    return list(map(lambda x: word_idx.get(x) if word_idx.get(x) is not None else len(word_idx) + 1,text))\n",
    "\n",
    "\n",
    "def make_dataset(texts, queries, nb_train_samples=None, num_neg_samples=5):\n",
    "    \n",
    "    # construct a dataset in a format of (context, query_positive, query_negative)\n",
    "    # assuming texts[i] maps to queries[i]\n",
    "    assert len(texts) == len(queries)\n",
    "    train_data = []\n",
    "    q_space = [q for subspace in queries for q in subspace]\n",
    "    \n",
    "    n = len(texts)\n",
    "    if nb_train_samples is not None:\n",
    "        n = nb_train_samples\n",
    "    \n",
    "    \n",
    "    for i in tqdm_notebook(range(n)):\n",
    "        for j in range(len(queries[i])):\n",
    "            negatives = sample_negatives(q_space, num_neg_samples)\n",
    "            for k in range(num_neg_samples):\n",
    "                train_data.append([texts[i], queries[i][j], negatives[k]])\n",
    "        \n",
    "    return train_data\n",
    "    \n",
    "        \n",
    "def sample_negatives(neg_space, n_samples):\n",
    "    # TODO: probs\n",
    "    return np.random.choice(neg_space, n_samples)\n",
    "\n",
    "\n",
    "\n",
    "def train(train_data, nb_epochs, test_size=0.3):\n",
    "    X_train, X_test = train_test_split(train_data, test_size=test_size, random_state=42)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8225/8225 [02:40<00:00, 51.19it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = parallelization(text2canonicals, data.Core.values, tq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.load('data/all_descriptions_keys.npy')\n",
    "\n",
    "queries = np.load('data/matched_keywords.npy', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = list(map(lambda x: list(map(lambda y: y.split(), x)), queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914840bd9db34d69ba95c4eee33a8e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "td = make_dataset(samples, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, q, _q = td[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1451]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.LongTensor([t]), torch.LongTensor([q]), torch.LongTensor([_q]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_queries = np.load('data/all_keywords_keys.npy')[1:]\n",
    "#all_queries = np.array(list(map(lambda x: x.split(), queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vocab = get_vocab(texts)\n",
    "query_vocab = get_queries_vocab(queries)\n",
    "\n",
    "context_word_idx = dict(zip(context_vocab, range(1, len(context_vocab) + 1)))\n",
    "query_word_idx = dict(zip(query_vocab, range(1, len(query_vocab) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastTextKeyedVectors.load(fasttext_model_path)\n",
    "context_emb_matrix = build_weight_matrix(fasttext, context_vocab)\n",
    "query_emb_matrix = build_weight_matrix(fasttext, query_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, context_encoder, query_encoder, context_dim, query_dim):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.context_encoder = context_encoder\n",
    "        self.query_encoder = query_encoder\n",
    "        \n",
    "        # siamese network arch\n",
    "        self.linear_1 = nn.Linear(context_dim + query_dim, 128)\n",
    "        self.linear_2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, context, query_pos, query_neg=None, train=True):\n",
    "        # take both queries while training and only one while testing to assign a score\n",
    "        # (second input just ignored if train=False)\n",
    "        context_repr = self.context_encoder(context)\n",
    "        query_pos_repr = self.query_encoder(query_pos)\n",
    "        siamese_inp_pos = torch.cat([query_pos_repr, context_repr], dim=-1)\n",
    "        score_pos = self.linear_2(self.linear_1(siamese_inp_pos))\n",
    "        \n",
    "        if train:\n",
    "            assert query_neg is not None, \"you have to provide a second input\" \n",
    "            query_neg_repr = self.query_encoder(query_neg)\n",
    "            siamese_inp_neg = torch.cat([query_neg_repr, context_repr], dim=-1)\n",
    "            score_neg = self.linear_2(self.linear_1(siamese_inp_neg))\n",
    "            return score_pos - score_neg\n",
    "        \n",
    "        else:\n",
    "            return score_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size=64):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding, num_embeddings, embedding_dim = self.create_emb_layer(emb_matrix)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=1,\n",
    "                          bidirectional=True, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': torch.tensor(weights_matrix)})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "\n",
    "        return emb_layer, num_embeddings, embedding_dim\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # X = app vector\n",
    "        embedded = self.embedding(X)\n",
    "        output, hn = self.gru(embedded)\n",
    "        return torch.cat([*hn], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_enc = Encoder(context_emb_matrix)\n",
    "query_enc = Encoder(query_emb_matrix)\n",
    "\n",
    "net = SiameseNetwork(context_enc, query_enc, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(map(lambda x: text_to_idx(x, context_word_idx), texts))\n",
    "queries = list(map(lambda x: list(map(lambda y: text_to_idx(y, query_word_idx), x)), queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#samples = pad_sequences(samples, 300)\n",
    "#queries = pad_sequences(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1957],\n",
       "        [-0.2361],\n",
       "        [-0.0274],\n",
       "        [ 0.0900],\n",
       "        [-0.0607],\n",
       "        [ 0.0679],\n",
       "        [-0.2605],\n",
       "        [-0.0944],\n",
       "        [ 0.1030],\n",
       "        [-0.1079]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(torch.LongTensor(samples[:10]), torch.LongTensor(queries[:10]), torch.LongTensor(queries[10:20]), train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
