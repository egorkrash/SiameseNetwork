{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from gensim.models.wrappers.fasttext import FastTextKeyedVectors\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "#from torch.utils.data import DataLoader\n",
    "import pickle as pkl\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "#data = pd.read_excel('data/task_10+kw_only_ru.xlsx', header=1)\n",
    "#data = pd.read_csv('new_apps_compatible.csv', sep=';').drop('Пустая колонка для совместимости', axis=1)\n",
    "fasttext_model_path = '/home/egor/Downloads/187/model.model'\n",
    "\n",
    "#Параллельно выполняющийся map функции func по массиву massive\n",
    "def parallelization(func, massive, jobs=3, tq=True):\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count() # Число наших ядер\n",
    "    if tq:\n",
    "        results = np.array(Parallel(n_jobs=num_cores)(delayed(func)(i) for i in tqdm(massive)))\n",
    "        return results\n",
    "    else:\n",
    "        results = Parallel(n_jobs=num_cores)(delayed(func)(i) for i in massive)\n",
    "        return results\n",
    "\n",
    "    \n",
    "def _word2canonical4w2v(word):\n",
    "    elems = morph.parse(word)\n",
    "    my_tag = ''\n",
    "    res = []\n",
    "    for elem in elems:\n",
    "        if 'VERB' in elem.tag or 'GRND' in elem.tag or 'INFN' in elem.tag:\n",
    "            my_tag = 'V'\n",
    "        if 'NOUN' in elem.tag:\n",
    "            my_tag = 'S'\n",
    "        normalised = elem.normalized.word\n",
    "        res.append((normalised, my_tag))\n",
    "    tmp = list(filter(lambda x: x[1] != '', res))\n",
    "    if len(tmp) > 0:\n",
    "        return tmp[0]\n",
    "    else:\n",
    "        return res[0]\n",
    "\n",
    "    \n",
    "def word2canonical(word):\n",
    "    return _word2canonical4w2v(word)[0]\n",
    "\n",
    "\n",
    "def getWords(text, filter_short_words=False):\n",
    "    if filter_short_words:\n",
    "        return filter(lambda x: len(x) > 3, re.findall(r'(?u)\\w+', text))\n",
    "    else:\n",
    "        return re.findall(r'(?u)\\w+', text)\n",
    "\n",
    "\n",
    "def text2canonicals(text, add_word=False, filter_short_words=True):\n",
    "    words = []\n",
    "    for word in getWords(text, filter_short_words=filter_short_words):\n",
    "        words.append(word2canonical(word.lower()))\n",
    "        if add_word:\n",
    "            words.append(word.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weight_matrix(word2vec, target_vocab, emb_dim=300):\n",
    "    matrix_len = len(target_vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            weights_matrix[i] = word2vec.get_vector(word)#word2vec[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    \n",
    "    return weights_matrix\n",
    "\n",
    "\n",
    "def get_vocab(texts):\n",
    "    return Counter([word for text in texts for word in text]).keys()\n",
    "\n",
    "\n",
    "def get_queries_vocab(queries):\n",
    "    return Counter([word for qs in queries for qury in qs for word in qury]).keys()\n",
    "\n",
    "\n",
    "def text_to_idx(text, word_idx):\n",
    "    return list(map(lambda x: word_idx.get(x) if word_idx.get(x) is not None else len(word_idx) + 1,text))\n",
    "\n",
    "\n",
    "def filter_zero_length(train_data):\n",
    "    i = 0\n",
    "    cnt = 0\n",
    "    while i < len(train_data):\n",
    "        t, q, _q = train_data[i]\n",
    "        if len(t) == 0 or len(q) == 0 or len(_q) == 0:\n",
    "            train_data.pop(i)\n",
    "            cnt += 1\n",
    "            i -= 1\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    print cnt, 'found'\n",
    "    return train_data\n",
    "\n",
    "\n",
    "def make_dataset(texts, queries, nb_train_samples=None, num_neg_samples=5):\n",
    "    # construct a dataset in a format of (context, query_positive, query_negative)\n",
    "    # assuming texts[i] maps to queries[i]\n",
    "    assert len(texts) == len(queries)\n",
    "    train_data = []\n",
    "    q_space = [q for subspace in queries for q in subspace]\n",
    "    # we have len(q_space) queries at all\n",
    "    # let's just sample all negatives in one run\n",
    "    negatives = np.random.choice(q_space, len(q_space) * num_neg_samples)\n",
    "    # now write it all into train data\n",
    "    k = 0\n",
    "    for i in tqdm_notebook(range(len(texts))):\n",
    "        for j in range(len(queries[i])):\n",
    "            z = 0\n",
    "            while k < len(negatives) and z < num_neg_samples:\n",
    "                train_data.append([texts[i], queries[i][j], negatives[k]])\n",
    "                k += 1\n",
    "                z += 1\n",
    "    \n",
    "    if nb_train_samples is not None:\n",
    "        return train_data[:nb_train_samples] \n",
    "    return train_data\n",
    "    \n",
    "    \n",
    "def sample_negatives(neg_space, n_samples):\n",
    "    # TODO: probs\n",
    "    return np.random.choice(neg_space, n_samples)\n",
    "\n",
    "\n",
    "def iterate_minibatches(inputs, batchsize, shuffle=False):\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "            batch = [inputs[x] for x in excerpt]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "            batch = inputs[excerpt]\n",
    "            \n",
    "        context, q_pos, q_neg = zip(*batch)\n",
    "    \n",
    "        clen = torch.cuda.LongTensor(list(map(len, context)))\n",
    "        qposlen = torch.cuda.LongTensor(list(map(len, q_pos)))\n",
    "        qneglen = torch.cuda.LongTensor(list(map(len, q_neg)))\n",
    "        \n",
    "        context = torch.cuda.LongTensor(pad_sequences(context, padding='post'))\n",
    "        q_pos = torch.cuda.LongTensor(pad_sequences(q_pos, padding='post'))\n",
    "        q_neg = torch.cuda.LongTensor(pad_sequences(q_neg, padding='post'))\n",
    "        \n",
    "        yield context, clen, q_pos, qposlen, q_neg, qneglen\n",
    "        \n",
    "# def get_data_loader(inputs, batch_size, shuffle=False):\n",
    "    \n",
    "#     context, q_pos, q_neg = zip(*inputs)\n",
    "#     clen = list(map(len, context))\n",
    "#     qposlen = list(map(len, q_pos))\n",
    "#     qneglen = list(map(len, q_neg))\n",
    "#     dataset = list(zip(context, clen, q_pos, qposlen, q_neg, qneglen)) # [context, clen, q_pos, qposlen, q_neg, qneglen]\n",
    "#     #return dataset[0]\n",
    "#     context = pad_sequences(context, padding='post')\n",
    "#     q_pos = pad_sequences(q_pos, padding='post')\n",
    "#     q_neg = pad_sequences(q_neg, padding='post')\n",
    "\n",
    "#     print(len(dataset))\n",
    "#     dataloader = DataLoader(dataset=context, batch_size=batch_size, shuffle=shuffle)\n",
    "#     return dataloader\n",
    "\n",
    "\n",
    "def train(train_data, batch_size, nb_epochs, test_size=0.3, shuffle=False, lr=0.001):\n",
    "    #X_train, X_test = train_test_split(train_data, test_size=test_size, random_state=42)    \n",
    "    loss_fn = lambda x: torch.mean(-F.logsigmoid(x)) # negative log likelihood\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    print('Start training...')\n",
    "    for epoch in range(nb_epochs):\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        generator = iterate_minibatches(train_data, batch_size, shuffle=shuffle)\n",
    "        for sample in generator:\n",
    "            context, clen, q_pos, qposlen, q_neg, qneglen = sample\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(context, clen, q_pos, qposlen, q_neg, qneglen)\n",
    "            loss = loss_fn(outputs)\n",
    "            #print 'loss', loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print statistics\n",
    "            running_loss += loss.item()#loss.data[0]\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "                \n",
    "            i += 1\n",
    "            \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8225/8225 [02:40<00:00, 51.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# texts = parallelization(text2canonicals, data.Core.values, tq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_queries = np.load('data/all_keywords_keys.npy')[1:]\n",
    "#all_queries = np.array(list(map(lambda x: x.split(), queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = np.load('all_descriptions_keys.npy')\n",
    "queries = np.load('matched_keywords.npy')\n",
    "queries = list(map(lambda x: list(map(lambda y: y.split(), x)), queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f292ddb7ebc402a980c603605f31856"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "692 found\n"
     ]
    }
   ],
   "source": [
    "context_vocab = get_vocab(texts)\n",
    "query_vocab = get_queries_vocab(queries)\n",
    "\n",
    "#context_word_idx = dict(zip(context_vocab, range(1, len(context_vocab) + 1)))\n",
    "#query_word_idx = dict(zip(query_vocab, range(1, len(query_vocab) + 1)))\n",
    "context_word_idx, query_word_idx = pkl.load(open('word_idx_dicts.pkl', 'rb'))\n",
    "\n",
    "samples = list(map(lambda x: text_to_idx(x, context_word_idx), texts))\n",
    "queries = list(map(lambda x: list(map(lambda y: text_to_idx(y, query_word_idx), x)), queries))\n",
    "\n",
    "train_data = make_dataset(samples, queries)\n",
    "train_data = filter_zero_length(train_data)\n",
    "np.random.shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext = FastTextKeyedVectors.load(fasttext_model_path)\n",
    "#context_emb_matrix = build_weight_matrix(fasttext, context_vocab)\n",
    "#query_emb_matrix = build_weight_matrix(fasttext, query_vocab)\n",
    "context_emb_matrix = np.load('context_emb_matr.npy')\n",
    "query_emb_matrix = np.load('query_emb_matr.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, context_encoder, query_encoder, context_dim, query_dim):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.context_encoder = context_encoder\n",
    "        self.query_encoder = query_encoder\n",
    "       \n",
    "        # siamese network arch\n",
    "        self.linear_1 = nn.Linear(context_dim + query_dim, 1024)\n",
    "        self.linear_2 = nn.Linear(1024, 1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "       \n",
    "    def forward(self, context, clens, query_pos, qposlens, query_neg=None, qneglens=None, train=True):\n",
    "        # take both queries while training and only one while testing to assign a score\n",
    "        # (second input just ignored if train=False)\n",
    "        context_repr = self.context_encoder(context, clens)\n",
    "        query_pos_repr = self.query_encoder(query_pos, qposlens)\n",
    "        siamese_inp_pos = torch.cat([query_pos_repr, context_repr], dim=-1)\n",
    "        score_pos = self.linear_2(self.linear_1(siamese_inp_pos))\n",
    "       \n",
    "        if train:\n",
    "            assert query_neg is not None, \"you have to provide a second input\"\n",
    "            query_neg_repr = self.query_encoder(query_neg, qneglens)\n",
    "            siamese_inp_neg = torch.cat([query_neg_repr, context_repr], dim=-1)\n",
    "            score_neg = self.linear_2(self.linear_1(siamese_inp_neg))\n",
    "            return score_pos - score_neg\n",
    "       \n",
    "        else:\n",
    "            return score_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size=64):\n",
    "        super(Encoder, self).__init__()\n",
    "       \n",
    "        self.embedding, num_embeddings, embedding_dim = self.create_emb_layer(emb_matrix)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=1,\n",
    "                          bidirectional=True, batch_first=True)\n",
    "       \n",
    "   \n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    " \n",
    "        return emb_layer, num_embeddings, embedding_dim\n",
    "       \n",
    " \n",
    "    def forward(self, X, X_lengths):\n",
    "        # X = app vector\n",
    "        with torch.no_grad():\n",
    "            embedded = self.embedding(X)\n",
    "       \n",
    "        embedded = torch.nn.utils.rnn.pack_padded_sequence(embedded, X_lengths, batch_first=True, enforce_sorted=False)\n",
    "        output, hn = self.gru(embedded)\n",
    "        #output, _ = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "        output = torch.cat([hn[0], hn[1]], dim=-1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_emb_matrix = torch.cuda.FloatTensor(context_emb_matrix)\n",
    "query_emb_matrix = torch.cuda.FloatTensor(query_emb_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (context_encoder): Encoder(\n",
       "    (embedding): Embedding(53160, 300)\n",
       "    (gru): GRU(300, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (query_encoder): Encoder(\n",
       "    (embedding): Embedding(16908, 300)\n",
       "    (gru): GRU(300, 64, batch_first=True, bidirectional=True)\n",
       "  )\n",
       "  (linear_1): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (linear_2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_enc = Encoder(context_emb_matrix)\n",
    "query_enc = Encoder(query_emb_matrix)\n",
    "\n",
    "net = SiameseNetwork(context_enc, query_enc, 128, 128)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iterate_minibatches(train_data[:10000], 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  523,   560,   641,  ...,     0,     0,     0],\n",
       "        [ 2956,  2957, 14435,  ...,   815,  3540,     0],\n",
       "        [  400,   317,  1497,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [ 6762,  1062,  5186,  ...,     0,     0,     0],\n",
       "        [  227,  5550,  2008,  ...,     0,     0,     0],\n",
       "        [   79,  2521, 19432,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6321],\n",
       "        [ 4.0733],\n",
       "        [ 4.4542],\n",
       "        [ 2.7738],\n",
       "        [ 6.5376],\n",
       "        [-0.6955],\n",
       "        [-2.6492],\n",
       "        [-0.7273],\n",
       "        [ 6.1513],\n",
       "        [-0.0266],\n",
       "        [-0.1195],\n",
       "        [ 1.4893],\n",
       "        [ 0.3598],\n",
       "        [ 0.9696],\n",
       "        [-1.5867],\n",
       "        [ 6.6357],\n",
       "        [ 2.7812],\n",
       "        [ 6.9800],\n",
       "        [10.9473],\n",
       "        [ 5.0718],\n",
       "        [-1.1097],\n",
       "        [ 7.9970],\n",
       "        [-0.1672],\n",
       "        [-0.5453],\n",
       "        [ 1.7603],\n",
       "        [-3.2352],\n",
       "        [ 0.9964],\n",
       "        [10.1794],\n",
       "        [ 1.1529],\n",
       "        [ 3.0358],\n",
       "        [ 5.1405],\n",
       "        [ 2.3718],\n",
       "        [ 0.5341],\n",
       "        [ 6.3462],\n",
       "        [18.2517],\n",
       "        [-1.5369],\n",
       "        [-1.8085],\n",
       "        [ 4.8054],\n",
       "        [-0.4793],\n",
       "        [-0.0184],\n",
       "        [ 2.4577],\n",
       "        [ 1.5279],\n",
       "        [ 3.2587],\n",
       "        [ 5.3874],\n",
       "        [ 2.3431],\n",
       "        [ 3.3633],\n",
       "        [-1.2965],\n",
       "        [ 2.1588],\n",
       "        [ 0.0718],\n",
       "        [ 7.0235],\n",
       "        [-5.5722],\n",
       "        [ 7.4561],\n",
       "        [-0.8987],\n",
       "        [ 2.4927],\n",
       "        [ 8.2291],\n",
       "        [-7.5326],\n",
       "        [ 2.1558],\n",
       "        [ 1.8934],\n",
       "        [ 0.4213],\n",
       "        [ 1.3285],\n",
       "        [-3.1029],\n",
       "        [-6.0197],\n",
       "        [ 4.3435],\n",
       "        [ 2.4674]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(sample[0], sample[1], sample[2], sample[3], train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1,   200] loss: 0.033\n",
      "[1,   400] loss: 0.020\n",
      "[1,   600] loss: 0.041\n",
      "[2,   200] loss: 0.025\n",
      "[2,   400] loss: 0.015\n",
      "[2,   600] loss: 0.042\n",
      "[3,   200] loss: 0.027\n",
      "[3,   400] loss: 0.015\n",
      "[3,   600] loss: 0.041\n",
      "[4,   200] loss: 0.028\n",
      "[4,   400] loss: 0.014\n",
      "[4,   600] loss: 0.041\n",
      "[5,   200] loss: 0.029\n",
      "[5,   400] loss: 0.014\n",
      "[5,   600] loss: 0.039\n",
      "[6,   200] loss: 0.030\n",
      "[6,   400] loss: 0.014\n",
      "[6,   600] loss: 0.037\n",
      "[7,   200] loss: 0.031\n",
      "[7,   400] loss: 0.015\n",
      "[7,   600] loss: 0.032\n"
     ]
    }
   ],
   "source": [
    "train(train_data[:50000], 64 , 10, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = iterate_minibatches(train_data[:20000], 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
