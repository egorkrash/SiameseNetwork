{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from gensim.models.wrappers.fasttext import FastTextKeyedVectors\n",
    "import keras.backend as K\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "data = pd.read_excel('data/task_10+kw_only_ru.xlsx', header=1)\n",
    "#data = pd.read_csv('new_apps_compatible.csv', sep=';').drop('Пустая колонка для совместимости', axis=1)\n",
    "fasttext_model_path = '/Users/egor/Downloads/187/model.model'\n",
    "\n",
    "#Параллельно выполняющийся map функции func по массиву massive\n",
    "def parallelization(func, massive, jobs=3, tq=True):\n",
    "    \n",
    "    num_cores = multiprocessing.cpu_count() # Число наших ядер\n",
    "    if tq:\n",
    "        results = np.array(Parallel(n_jobs=num_cores)(delayed(func)(i) for i in tqdm(massive)))\n",
    "        return results\n",
    "    else:\n",
    "        results = Parallel(n_jobs=num_cores)(delayed(func)(i) for i in massive)\n",
    "        return results\n",
    "\n",
    "def _word2canonical4w2v(word):\n",
    "    elems = morph.parse(word)\n",
    "    my_tag = ''\n",
    "    res = []\n",
    "    for elem in elems:\n",
    "        if 'VERB' in elem.tag or 'GRND' in elem.tag or 'INFN' in elem.tag:\n",
    "            my_tag = 'V'\n",
    "        if 'NOUN' in elem.tag:\n",
    "            my_tag = 'S'\n",
    "        normalised = elem.normalized.word\n",
    "        res.append((normalised, my_tag))\n",
    "    tmp = list(filter(lambda x: x[1] != '', res))\n",
    "    if len(tmp) > 0:\n",
    "        return tmp[0]\n",
    "    else:\n",
    "        return res[0]\n",
    "\n",
    "    \n",
    "def word2canonical(word):\n",
    "    return _word2canonical4w2v(word)[0]\n",
    "\n",
    "\n",
    "def getWords(text, filter_short_words=False):\n",
    "    if filter_short_words:\n",
    "        return filter(lambda x: len(x) > 3, re.findall(r'(?u)\\w+', text))\n",
    "    else:\n",
    "        return re.findall(r'(?u)\\w+', text)\n",
    "\n",
    "\n",
    "def text2canonicals(text, add_word=False, filter_short_words=True):\n",
    "    words = []\n",
    "    for word in getWords(text, filter_short_words=filter_short_words):\n",
    "        words.append(word2canonical(word.lower()))\n",
    "        if add_word:\n",
    "            words.append(word.lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weight_matrix(word2vec, target_vocab, emb_dim=300):\n",
    "    matrix_len = len(target_vocab)\n",
    "    weights_matrix = np.zeros((matrix_len, emb_dim))\n",
    "    words_found = 0\n",
    "\n",
    "    for i, word in enumerate(target_vocab):\n",
    "        try: \n",
    "            weights_matrix[i] = word2vec[word]\n",
    "            words_found += 1\n",
    "        except KeyError:\n",
    "            weights_matrix[i] = np.random.normal(scale=0.6, size=(emb_dim, ))\n",
    "    \n",
    "    return weights_matrix\n",
    "\n",
    "def get_vocab(texts):\n",
    "    return Counter([word for text in texts for word in text]).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8225/8225 [02:40<00:00, 51.19it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = parallelization(text2canonicals, data.Core.values, tq=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pkl.load(open('data/texts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = np.load('data/all_keywords_keys.npy')[1:]\n",
    "queries = np.array(list(map(lambda x: x.split(), queries)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl.dump(texts, open('texts.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext = FastTextKeyedVectors.load(fasttext_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = get_vocab(texts)\n",
    "query_vocab = get_vocab(queries)\n",
    "context_emb_matrix = build_weight_matrix(fasttext, target_vocab)\n",
    "query_emb_matrix = build_weight_matrix(fasttext, query_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = nn.Embedding(*weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,2,3])\n",
    "\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs, num_embeddings, embedding_dim = create_emb_layer(weight_matrix, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2697, -0.5741, -1.5527, -2.2854, -1.2620, -0.5197,  3.0912,  0.7927,\n",
       "         -0.6073,  2.3412,  0.3576,  2.0995, -0.9263, -0.5159, -2.0464, -0.4677,\n",
       "         -2.9064, -0.0943,  0.6719,  2.4594,  1.6578, -2.9194, -0.0200,  1.2851,\n",
       "         -2.8199, -0.4918,  0.2292, -0.9744, -3.2194, -3.5566, -1.6003,  0.2985,\n",
       "          0.5833, -0.0737,  1.0696, -2.0138, -0.1739,  0.6468,  0.0755,  0.0971,\n",
       "         -0.1788,  2.0031,  0.4137, -2.9208,  1.8085,  1.6108,  1.3114, -1.9675,\n",
       "         -1.4978, -2.3921, -0.3813,  0.7139,  1.5572,  0.0862, -0.5874,  1.2259,\n",
       "         -2.1570,  0.3506,  0.5972,  0.7138,  2.2535, -0.2766,  0.2211, -0.9814,\n",
       "          0.5154, -0.7607,  0.3884,  3.5891,  1.7439,  0.7744,  1.3007,  1.2588,\n",
       "          1.2458, -1.9587,  0.3296,  1.2414, -1.3811, -2.7530, -0.3852,  1.8004,\n",
       "         -1.3537,  3.0254,  3.0503, -1.7619, -1.0446,  0.6383, -0.6296,  2.4570,\n",
       "          0.9960,  1.8724,  2.5050, -0.1670, -0.1157, -1.9065, -0.1082, -1.3729,\n",
       "          0.9684, -0.8983, -2.1295,  2.3596,  0.5286, -0.8339,  0.8847, -0.8947,\n",
       "         -0.2970, -2.9383, -2.0401, -0.1072,  0.1767, -0.9777,  0.1852,  0.0653,\n",
       "         -0.1692,  1.2026, -1.1932, -0.9227,  0.8569, -0.2739,  0.7042, -0.4209,\n",
       "         -1.8591,  2.7543, -1.4468, -1.2337,  0.9265, -0.1541,  2.6621,  3.0925,\n",
       "          3.4813, -0.3823, -4.6656, -0.3262,  0.6844,  3.2651,  3.7587,  2.6792,\n",
       "          1.8201,  0.1066,  1.9819, -3.4336,  0.0933,  1.7368,  1.6731, -1.6545,\n",
       "         -0.8069, -4.4324, -1.5860, -1.4034, -0.0092,  3.2547, -1.7804, -2.0134,\n",
       "         -0.3098,  0.5083,  1.2866,  0.7065,  2.4816,  0.6033, -1.5546,  1.6743,\n",
       "          0.7450, -0.8206,  2.0991,  0.8286, -0.8718,  1.5238, -2.6274, -3.3366,\n",
       "         -1.0323,  0.3302,  2.0419, -5.4879,  1.0910, -2.3857, -1.1168, -1.1759,\n",
       "         -3.0540, -2.6575,  1.8771, -0.8182, -3.2663,  0.6820, -0.3890,  3.4434,\n",
       "         -0.1065, -1.1168, -2.4792, -1.8577,  2.2806, -0.0117, -0.7818,  3.0726,\n",
       "          0.3839,  0.4122, -0.0732, -0.5188, -1.0816,  1.7733,  0.8768,  3.4274,\n",
       "         -2.4430, -1.7856,  1.2754,  0.0493, -1.0097, -2.0855,  0.2011, -2.6895,\n",
       "         -0.0430, -0.7537, -2.0447, -0.3346, -0.0890,  0.7783, -1.5845,  2.5534,\n",
       "          2.8786, -1.5956,  0.4594, -0.0194,  1.1815, -0.7911, -1.7289, -0.6680,\n",
       "          0.3703, -1.3628, -4.3842,  0.8236,  2.8749,  1.1459, -2.2303,  0.4468,\n",
       "         -3.1692,  1.0073, -0.9684, -1.6451,  1.2220, -0.8635, -0.6051, -3.5323,\n",
       "          1.2412, -0.9808,  0.3650, -0.6896,  0.0543, -0.1504,  0.3493, -0.2908,\n",
       "         -1.4240, -2.6101, -1.8228, -0.7322, -0.1610, -1.4315, -1.0791, -1.1471,\n",
       "         -0.0969,  0.8320,  3.4340,  0.7755, -3.3689,  1.2420, -0.0323, -2.1087,\n",
       "         -0.7937,  1.4400,  1.0089,  0.8324, -0.1046, -0.7837, -0.3487, -0.0556,\n",
       "         -0.8099, -2.2838, -0.6967,  0.9695, -0.5420,  1.8329, -1.4958, -1.6715,\n",
       "          0.1392,  2.0382,  1.0220,  0.7149, -0.3715,  1.7661, -1.5745, -0.3675,\n",
       "          0.7683,  0.5987,  0.5927,  1.5554, -1.1504,  1.0613, -1.7157,  0.5797,\n",
       "         -1.4331,  1.6347, -0.9814,  1.1417]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.GRU(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn = rnn(input, h0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, context_dim, query_dim):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(context_dim + hidden_size * 2, 128)\n",
    "        self.linear_2 = nn.Linear(128, 1)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, context, query_pos, query_neg):\n",
    "        # takes two inputs. first is the hidden representation of app description and the second is the queries batch\n",
    "        _, q_repr_pos = self.query_gru(self.query_emb(query_pos))\n",
    "        q_repr_pos = torch.cat([*q_repr_pos], dim=-1)\n",
    "        \n",
    "        _, q_repr_neg = self.query_gru(self.query_emb(query_neg))\n",
    "        q_repr_neg = torch.cat([*q_repr_neg], dim=-1)\n",
    "        print(q_repr_neg.shape)\n",
    "        siamese_inp_pos = torch.cat([q_repr_pos, context], dim=-1)\n",
    "        siamese_inp_neg = torch.cat([q_repr_neg, context], dim=-1)\n",
    "        \n",
    "        score_pos = self.linear_2(self.relu(self.linear_1(siamese_inp_pos)))\n",
    "        score_neg = self.linear_2(self.relu(self.linear_1(siamese_inp_neg)))\n",
    "        \n",
    "        return score_pos - score_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_matrix, hidden_size=64):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding, num_embeddings, embedding_dim = self.create_emb_layer(emb_matrix)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def create_emb_layer(self, weights_matrix, non_trainable=False):\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        emb_layer.load_state_dict({'weight': torch.tensor(weights_matrix)})\n",
    "        if non_trainable:\n",
    "            emb_layer.weight.requires_grad = False\n",
    "\n",
    "        return emb_layer, num_embeddings, embedding_dim\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        # X = app vector\n",
    "        embedded = self.embedding(X)\n",
    "        output, hn = self.gru(embedded)\n",
    "        return torch.cat([*hn], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_enc = ContextEncoder(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "net = SiameseNetwork(len(target_vocab), 128, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 382), dtype=int32)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([0, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 1. Got 0 and 1 in dimension 0 at ../aten/src/TH/generic/THTensor.cpp:689",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-432-71aa6bd87217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-425-2f01d97e3d9b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, context, query_pos, query_neg)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_repr_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0msiamese_inp_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_repr_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msiamese_inp_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_repr_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mscore_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msiamese_inp_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 0 and 1 in dimension 0 at ../aten/src/TH/generic/THTensor.cpp:689"
     ]
    }
   ],
   "source": [
    "net(context_vec[:1], torch.tensor(samples[:1], dtype=torch.long), torch.tensor(samples[10:11], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = dict(zip(target_vocab, range(1, len(target_vocab) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_ids(text):\n",
    "    return list(map(lambda x: word_idx[x],text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = list(map(text_to_ids, texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pad_sequences(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec = context_enc(torch.tensor(samples, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2355,  0.3443,  0.8532,  ...,  0.9841,  0.8898,  0.9152],\n",
       "        [ 0.2514,  0.0933, -0.1488,  ...,  0.9848,  0.9995,  0.9163],\n",
       "        [-0.4006, -0.3471, -0.8224,  ...,  0.9850,  0.9995,  0.9163],\n",
       "        ...,\n",
       "        [-0.4236, -0.9753, -0.8926,  ...,  0.9846,  0.9996,  0.9163],\n",
       "        [-0.6892, -0.7874, -0.5873,  ...,  0.9846,  0.9993,  0.9160],\n",
       "        [ 0.4210, -0.9002, -0.2217,  ..., -0.0679, -0.8469,  0.2181]],\n",
       "       grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_h = output[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8601, -0.9466,  0.7250, -0.6369,  0.9348, -0.0059,  0.9198,  0.1797,\n",
       "         0.9451, -0.9606,  0.9995,  0.5406,  0.9725, -0.4963,  0.9374,  0.9493,\n",
       "         0.9917, -0.9907, -0.9998,  0.9675,  0.1656, -0.7281,  0.2684,  0.9403,\n",
       "        -0.9107, -0.4866, -0.6494,  0.9715,  0.9971, -0.1840,  0.8659,  0.0870,\n",
       "        -0.8100,  0.4066, -0.9718, -0.9794,  0.9616,  0.5550, -0.9980, -0.9904,\n",
       "        -0.9829, -0.7062, -0.5788, -0.8205, -0.9125, -0.9986,  0.9630, -0.9921,\n",
       "        -0.9903,  0.9998,  0.9872,  0.9989,  0.1507, -0.1279, -0.9667,  0.9838,\n",
       "        -0.2132,  0.8981,  0.7400,  0.6980,  0.9585,  0.9114, -0.6787, -0.9613],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3220, -0.7119,  0.9637, -0.1117, -0.4310,  0.5825,  0.3635, -0.7355,\n",
       "        -0.4684,  0.4242, -0.8651, -0.1600,  0.2515, -0.3122, -0.2554,  0.7875,\n",
       "        -0.5054,  0.2652, -0.5298, -0.6273, -0.2431,  0.0521,  0.5271,  0.0659,\n",
       "         0.6781, -0.9272,  0.1873,  0.3154,  0.6975, -0.2104, -0.0027,  0.4963,\n",
       "        -0.5183, -0.6684, -0.2173,  0.4662,  0.8450, -0.8730, -0.9728, -0.2313,\n",
       "        -0.2085, -0.7575, -0.4352,  0.2833, -0.5789,  0.6157, -0.3858,  0.4125,\n",
       "        -0.8934, -0.9073, -0.1653, -0.1109, -0.7207, -0.3492, -0.1671, -0.0198,\n",
       "         0.4761,  0.4061, -0.8034,  0.3017,  0.8329,  0.6911,  0.5665, -0.2176],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([*hn], dim=-1)[:, 64:][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.8601, -0.9466,  0.7250, -0.6369,  0.9348, -0.0059,  0.9198,  0.1797,\n",
       "         0.9451, -0.9606,  0.9995,  0.5406,  0.9725, -0.4963,  0.9374,  0.9493,\n",
       "         0.9917, -0.9907, -0.9998,  0.9675,  0.1656, -0.7281,  0.2684,  0.9403,\n",
       "        -0.9107, -0.4866, -0.6494,  0.9715,  0.9971, -0.1840,  0.8659,  0.0870,\n",
       "        -0.8100,  0.4066, -0.9718, -0.9794,  0.9616,  0.5550, -0.9980, -0.9904,\n",
       "        -0.9829, -0.7062, -0.5788, -0.8205, -0.9125, -0.9986,  0.9630, -0.9921,\n",
       "        -0.9903,  0.9998,  0.9872,  0.9989,  0.1507, -0.1279, -0.9667,  0.9838,\n",
       "        -0.2132,  0.8981,  0.7400,  0.6980,  0.9585,  0.9114, -0.6787, -0.9613],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 0, 64:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8493,  0.6618,  0.3642,  ..., -0.4162, -0.9182,  0.1056],\n",
       "         [-0.4135, -0.8876, -0.1321,  ..., -0.3913, -0.7934,  0.8455],\n",
       "         [ 0.0877,  0.8097, -0.1604,  ..., -0.4779, -0.2237,  0.5258],\n",
       "         ...,\n",
       "         [ 0.7312, -0.7872, -0.7111,  ..., -0.3504, -0.3364, -0.5374],\n",
       "         [ 0.4145, -0.6863,  0.0040,  ..., -0.2079, -0.7165,  0.2850],\n",
       "         [ 0.4713, -0.6098,  0.2443,  ...,  0.5707,  0.2151,  0.8724]],\n",
       "\n",
       "        [[ 0.8601, -0.9466,  0.7250,  ...,  0.9114, -0.6787, -0.9613],\n",
       "         [ 0.8664, -0.9466,  0.2125,  ...,  0.9149, -0.6854, -0.9614],\n",
       "         [ 0.8669, -0.9466, -0.2779,  ...,  0.9091, -0.6775, -0.9615],\n",
       "         ...,\n",
       "         [ 0.8663, -0.9466,  0.2378,  ...,  0.9158, -0.6866, -0.9614],\n",
       "         [ 0.8674, -0.9467, -0.6760,  ...,  0.9055, -0.6742, -0.9616],\n",
       "         [-0.3220, -0.7119,  0.9637,  ...,  0.6911,  0.5665, -0.2176]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1923,  0.0825, -0.1506,  ...,  0.9114, -0.6787, -0.9613],\n",
       "        [ 0.1923,  0.0825, -0.1506,  ...,  0.9149, -0.6854, -0.9614],\n",
       "        [ 0.1923,  0.0825, -0.1506,  ...,  0.9091, -0.6775, -0.9615],\n",
       "        ...,\n",
       "        [ 0.1923,  0.0825, -0.1506,  ...,  0.9158, -0.6866, -0.9614],\n",
       "        [ 0.1923,  0.0825, -0.1506,  ...,  0.9055, -0.6742, -0.9616],\n",
       "        [-0.0946,  0.3019,  0.0660,  ...,  0.6911,  0.5665, -0.2176]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
